MapReduce:

-   Users implement interface of two functions:

    -   map (in_key, in_value) ->
    (inter_key, inter_value) list

    -   reduce ((inter_key, inter_value) list) ->
    (out_key, out_value) list

-   Map phase:

    -   Example 1: Explode Mapper

        Explode function using the map method where 

        let map(k,v) = foreach char c in v:
            foreach char in v:
                emit(k,c)

        ("A", "cats") -> ("A", "c"), ("A", "a"),
                            ("A", "t"), ("A", "s")

    -   Example 2: Filter Mapper
    
        let map(k,v) = 
            if (isPrime(v)) then emit(k, v)

        ("foo", 7) -> ("foo", 7)
        ("test", 10) -> (nothing)

-   Reduce phase   

    After the map phase is over, all the intermediate values for a given
    output key are combined together into a list.

    reduce() combines those intermediate values into one or more final
    values for that same output key.

    Example: Sum Reducer

    let reduce(k, vals) = 
        sum = 0
        foreach int v in vals:
            sum +=v
        emit(k,sum)

    ("A", [42, 100, 312] -> ("A", 454)
    ("B", [12, 6, -2] -> ("B", 16)

-   Parallism is the key of map-reduce

    1)  map() functions run in parallel, creating different
        intermediate values from different input data sets.
        
    2)  reduce() functions also run in parallel, each working on a
        different output key

    3)  All values are processed independently

    4)  Bottlenect: reduce phase can't start until map phase is completely finished.

    Let's take a look at the wordcount example:

    map(String input_key, String input_value) :
        //input_key : document name
        //input_value : document contents
        for each word w in input_value :
            emit(w,1)

    reduce(String output_key, Interator<int>intermediate_values) :
        //output_key : a word
        //output_values: a list of counts
        int result = 0;
        for each v in intermediate_values:
            result +=v;
        emit(output_key, result)

-   Combining phase

    -   Run on mapper nodes after map phase;
    -   "Mini-reduce", only on local map output;
    -   Used to save bandwidth before sending data to full reducer;
    -   Reducer can be combiner if commutative & associative
        - e.g. SumReducer

-   The Hadoop Distributed File System(HDFS)

    -   Based on Google's GFS
    -   Redundant storage of massive amounts of data on cheap
        and unreliable computers
    -   Why not use an existing file system?

        -   Different workload and design priorities;
        -   Handles much bigger dataset sizes than other file systems

    -   Assumptions

    1) High component failure rates;
    2) "Modest" number of HUGE files
    
        -   Just a few million
        -   Each is 100MB or large; multiple-GB files typical
    3)  Files are write-once, mostly appended to
    4)  Large streaming reads
    5)  High sustained thoughtput favored over low latency

    -   Features of HDFS

    -   Files stored as blocks

        -   Much larger size than most filesystems (default is 64MB)
        -   Reliability through replication

            -   Each block replicated across 3+ DataNodes;

        -   Single master(NameNode) coordinates access, metadata
            -   Simple centralized management;

        -   No data caching
            -   Little benefit due to large data sets, streaming reads
        
        -   Familiar interface, but customize the API
            -   Simplify the problem; focus on distributed apps

-   99% big data interview questions killer

    -   What is big data processing

        Big data processing is nothing but about the storage, processing and operation on huge amount of data. Because the data volumn is so huge therefore
        there is no way to finish processing in short time or cannot those data in memory at one take.

    -   thumb of rule

        divide and conquer!

        there are a couple methods to deal with big data processing:

        1.  divide and conquer/hashmap + hash stastics + hash/quick/merge sort
        2.  Double bucket partition
        3.  Bloom filter/Bigmap
        4.  Trie Tree/Database/倒排索引(??)
        5.  外排序(??)
        6.  Mapreduce/Hadoop

    -   Part 1: from set/map to hashtable/hash_map/hash_set

        -   Introduction

            There two types of STL(ISO standard type library)

            1)  Sequential Containers

                vector/list/deque/stack/queue/heap

            2)  Relational Containers

                RB-tree
                |
                v
                set 
                |
                v
                multiset

                RB-tree
                |
                v
                map(using RB-tree):
                |
                v
                multimap

                hash
                |
                v
                hashtable-----> hash_multimap
                |       |    
                v       ----> hash_multiset
                hash_map

                What is relational container? Similar to relational database, every data unit or element has a pair of a key and a value. When inserting an element, RB-tree or
                hash table uses a specific rule that is based on the key value to put that element into a proper position.

        -   set/map/multiset/multimap

            1) set and map are both based on the data structure called RB-Tree

               same: both do not allow two elements have the same value(for set) or key (for map)
               diff: set only has value but not key; map has both value and key

               multiset/multimap are very similar to set/map execpts that they allow repetitive value(for set)/key(for map)(??). That is all the insert operations are based on
               RB-Tree's insert_equal() but not insert_unique()(??).

            2) hash_set/hash_map/hash_multiset/hash_multimap

               Unlike set/map, all hash data structures are based on hash table. Therefore, there is no "auto-sorting" feature in them.


        Part 2:  divide and conquer/hashmap + hash stastics + hash/quick/merge sort

                -   Problem 1: 

                    Given a big file that each row has one ip address, find out the most frequent one.

                    Step 1: divide

                    Because the data is too big and the memory is very limited, the only thing we can do is to divide the big file into smaller files

                    Step 2: hashmap

                    Now we have smaller files. For each of those smaller files, we can use hash_map(ip, value) to do freq stastics

                    Step 3: heap/quick sorting

                    We can use a specific algorithm to sort those ip addresses.

                    In practice, we can use %1000 to divide the whole file into 1000 smaller files. Noted that every ip address is 32-bit long.

                    There are a couple things to note:

                    1) Mod 1000 forces the same ip address only exist in a single file

                -   Problem 2: (hashmap + heap)

                    Each query is nothing but a string of characters. Amazon has logging problem. Each log file has history of all system access records and 
                    each access record has size from 1 to 255 bytes. Assuming we have 10 millions records but there are repetitions (3millions distinct records). 
                    You need to find out the 10 most popular records for all queries and system has just 1GB memory. 

                    Solution: 

                    Even though there are 10 million queries, but there are 3 million repetitions: we can think about if we can put the 3 million entries in
                    the memory. At Maximum, it takes: 3 million * 255 bytes = 0.75GB. 

                    Step1: Maintain a hash_map (Query, Value), read each query, if the query is not in the map table, then add map(query, 1) into the map table; if the
                    query exists in the map table, do: (query, value++). This step takes O(n) time complexity, where n = 10 million.

                    step2: do heap sorting

                    Maintain a size of k = 10 small root heap. Traverse through all of those 3 million hash_map elements and compare them to
                    the top 10 heap roots. In practice, We can follow the following steps:
        
                    2.1 Put the first k=10 elements into the heap, and do "buildminheap". So now, the root of the heap is the smallest node and all the elements are
                        satisfy with the min heap tree requirement.
                    2.2 Insert the rest elements(3 million-k) in to the heap. If inserting element is larger than the root, use it to replace the root, then
                        do minheapify(0). 
                    
                    The final k elements in the heap are now the biggest elements. step 2 takes O(n'*log(k)) where n'=3 million. 

                    step1 + step2 = O(n) + O(n'*log(k)) = O(n*log(k)).

                    We take advantage of the small heap in step 2.2 where each insert operation takes log(k).(!!). For the actual implementation, please refer to
                    the folder "sortqueries".

                -   Problem 3: 

                    Within a file which is 1GB, each line is nothing but a word. Each word is less equal or less than 16 bytes. Your machine only has 1 MB ram. Get the
                    top 100 highest frequent words.

                    

                    

                    
