Chapter 1: The Role of Algorithms in Computing

-  What is algorithm?

   Informally, an algorithm is any well-defined computational procedure that takes
   some value, or set of values, as input and produces some value, or set of values, as
   output. An algorithm is thus a sequence of computational steps that transform the
   input into the output.

-  What is data structure?

   A data structure is a way to store and organize data in order to facilitate access
   and modifications.
   
Chapter 2: Getting started

-  One example: Insert Sort ( it takes time roughly equal to c1*n.^2 )

   for j = 2 to A.length
      key = A[j]
      /* Insert A[j] into the sorted sequence A[1...j-1] */
      i = j - 1
      while i > 0 and A[i] > key
         A[i+1] = A[i]
         i = i - 1
      A[i+1] = key   <---This line doesn't belong to the above while loop (line 23 to line 25)

-  Loop variants

   Loop variants are to help us understand why an algorithm is correct:

   Initialization: It is true prior to the first iteration of the loop.

   Maintenance: If it is true before an iteration of the loop, it remains true before the next iteration.

   Termination: When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.

-  2.1-3

   Input:   A sequence of n numbers A = <a1, a2, ..., an> and a value v.
   Output:  An index i such that v = A[i] or the special value NIL if v does not appear in A

   Write pseudocode for linear search, which scans through the sequence, looking
   for. Using a loop invariant, prove that your algorithm is correct. Make sure that
   your loop invariant fulfills the three necessary properties.

   v = NIL
   for i = 1 to A.length
      if v = A[i]
      v = A[i]
      return
   
   Initialization: v=NIL, i = 1.
   Maintenance: i<=A.length, or, v != A[i] while i <= A.length.
   Termination: v=A[i] while i < A.length+1, or, i=A.length+1

-  Analyzing algorithms

   Running time: the running time of an algorithm on a particular input is the number of primitive operations or "steps" executed. 

   Loop running time: When a for or while loop exits in the usual way (i.e. due to the test in the loop header), the test is executed one time more than the loop body.

   Worst case running time: We shall usually concentrate on finding only the worst-case running time, that is, the longest running time for any input of size n.

   Order of growth: To make analizing algorithms more simplified, we use the rate of growth,or order of growth, of the running time that really interests us. E.g.

   Insert Sort's running time (of course in the worst case)is a*n.^2 + b*n + c. For a given large value of n, b*n and c are relatively insignificant so we consider only
   the leading term of a formula: a*n.^2. We also ignore the leading term's constant coefficient, since constant factors are less significant for large values of n. In summary,
   We use "theata of n-squared" to represent the running time of insert sort: O(n.^2). 

   We usually consider one algorithm to be more efficient than another if its worst-case running time has a lower order of growth. Due to constant factors and lower-order terms,
   an algorithm whose running time has a higher order of growth might take less time for small inputs than an apgorithm whose running time has a lower order of growth. But for large
   inputs, a O(n.^2) algorithm, for example, will run more quickly in the worst case than a O(n.^3) algorithm.

-   Growth of Functions

    -   Asymptotic notation

        We will use asympotic notation primarily to describe the running time of algorithms.

    -   Delta notation (asymptotically tight bound)

        delta(g(n)) = {f(n): there exists c1, c2 and n0 such that c1*g(n)<=f(n)<=c2*g(n) for all n >= n0}

        f(n) is sandwiched between c1*g(n) and c2*g(n)

        The definition of delta(g(n)) requires that every member f(n) be asymptotically nonnegative consequently the function g(n) itself must be asymptotically
        nonnegative.

        For any quadratic function f(n) = a.^2*n + b*n +c, formally we take the constant c1=a/4 and c2=7a/4 and n0=2*max(|b|/a, sqrt(|c|/a))

    -   Big-O notation (asymptotically upper bound)

        O(g(n)) = {f(n): there exists c n0 such that 0<=f(n)<=c*g(n) for all n >= n0}

        -   o-notation

            The bound 2*n.^2 = O(n.^2) is asymptotically tight, but the bound 2*n = O(n.^2) is not. We use o-notation to denote an upper bound that is not
            asymptotically tight("little-omega of g of n").

            o(g(n)) = {f(n): for any positive constant c > 0, there exists a constant n0 > 0 such that 0<=f(n)<c*g(n) for all n>=n0}.

            for example, 2n = o(n.^2), but 2n.^2 != o(n.^2).

    -   Omega notation (asymptotically upper bound)

        O(g(n)) = {f(n): there exists c n0 such that 0<=c*g(n)<=f(n) for all n >= n0}

        -   w-notation

            We use w-notation to denote a lower bound that is not asymptotically tight:

            w(g(n)) = {f(n): for any positive constant c > 0, there exists a constant n0 > 0 such that 0<=c*g(n) < f(n) for all n>=n0}.

            for example, n.^2/2 = w(n), but n.^2/2 != w(n.^2).

            
        

-  Designing algorithms

   The divide-and-conquer approach: Recursion functions are one of those functions that can call itself recursively one or more time to deal with closely related sub problems.

   Divide: divide the problem into a number of subproblems that are smaller instances of the same problem.
   
   Conquer: Conquer the subproblems by solving them recursively. If the subproblems are sizes are small enough, however, just solve the subproblems in a straightforward manner.

   Combine: Combine the solutions to the subproblems into the solution for the original problem.

   E.g. Merge sort:

   Divide: Divide the n-element sequence to be sorted into two subsequences of n/2 elements each.
   Conquer: Sort the two subsequences recursively using merge sort.
   Combine: Merge the two sorted subsequences to produce the sorted answer.

   *  Merge:

   1   MERGE(A, p, q, r)
   1   n1 = q-p+1
   2   n2 = r-q
   3   Let L[1, ...n1] and R[1, n2] be the new two arrays.
   4   for i = 1 to n1
   5      L[i] = A[p-1+i]
   6   for j = 1 to n2
   7      R[j] = A[q+j]
   8   L[n1+1]=inf
   9   R[n2+1]=inf
   10  i = 1
   11  j = 1
   12  for k = p to r
   13    if L[i]<=R[j]
   14       A[k] = L[i]
   15       i = i + 1
   16    else
   17       A[k] = R[j]
   18       j = j + 1

   *  Merge_Sort:

   MERGE-SORT(A, p, r)
 
   1  if (p < r)
   2  q = floor((p+r)/2)
   3  MERGE-SORT(A, p, q)
   4  MERGE-SORT(A, q+1, r)
   5  MERGE(A, p, q, r)

   Typically, we set p = 1 and r = n. Now we analyze divide-and-conquer algorithms.

   When an algorithm contains a recursive call to itself, we can often describe its running time by a recurrence equation or recurrence, which describes the overall running time
   ona a problem of size n in terms of the running time on smaller inputs. 

   Tn = o(1)                           if n <= c
      = a*T(n/b) + D(n) + C(n)         otherwise
   
   1) If the problem is smaller enough (e.g. sorting two numbers), say n<=c, for some constant c, the straightforward solution takes constant time, which we write o(1).
   2) Suppose that our division of the problem yields a subproblems, each of which is 1=b the size of the original. (For merge sort, both a and b are 2, each of which is 
   the size of the original. (For merge sort, both  andbut we shall see many divide-and-conquer algorithms in which a ¤ b.) time T .n=b/ to solve one subproblem of size 
   n=b, and so it takes time a*T(n/b).
   3) If we take D(n) time to divide the problem into subproblems and C(n) time to combine the solutions to the subproblems into the original problem. 

   * Analysis of Merge Sort

   Read pages 36 to 38. The rate of growth is O(n*lg(n))

Chapter 3: Growth of functions

   -  Asymptotic efficiency of an algorithm:

      That is how the running time of an algorithm increases with the size of the input n in the limit, as the size of the input increases without bound. 

   -  Θ(Theta)-notation

      Θ(g(n)) = {f(n): there exist positive constants c1, c2, and n0 such that 0<=c1g(n)<=f(n)<=c2g(n) for all n >= n0}.

   -  Big O-notation

      O(g(n)) = {f(n): there exist positive constant c and n0 such that 0<=f(n)<=cg(n) for all n >= n0}

   -  Ω(Omega)-notation

      O(g(n)) = {f(n): there exist positive constant c and n0 such that 0<=cg(n)<=f(n) for all n >= n0}
      

Chapter 4: Divide-and-Conquer

   This chapter fouses on three methods for solving recurrences - that is, for obtaining asymptotic "O" bounds on the solution.

   -  Recurrences:

   A recurrence is an equation or inequality that describes a function in terms of its value on smaller inputs. For example, the worst-case running time T(n) of the MERGE-SORT
   produce by recurrence:

   T(n) = o(1) if n = 1;
        = 2T(n/2) + o(n) if n > 1

   whose solution we claim to be:

   T(n) = o(n*lgn)

   1) Method 1: substitution method. We guess a bound and then use mathematical induction to prove our guess correct.

   2) Method 2: recursion-tree method. converts the recurrence into a tree whose nodes represent the costs incurred at 
   various levels of the recursion. We use techniques for bounding summations to solve the recurrence.

   3) Method3 3: master method provides bounds for recurrences of the form
   T(n) = a*T(n/b) + f(n)
   
   where a >= 1, b > 1, and f (n) is a given function. A recurrence of this form characterizes a divide-and-conquer algorithm that
   creates a subproblems, each of which is 1/b the size of the original problem, and in which the divide and combine steps together
   take f(n) time.

   Example: Find the max values of sub-array:

   FIND-MAX-CROSSING-SUBARRAY(A, low, mid, high):

   1 left-sum = -inf
   2 sum = 0
   3 for i = mid downto low
   4     sum = sum + A[i]
   5     if sum > left-sum
   6        left-sum = sum
   7        max-left = i
   8 right-sum = -inf
   9 sum = 0
   10 for j = mid + 1 to high
   11 sum = sum + A[j]
   12    if sum > right-sum
   13       right-sum = sum
   14       max-right = j
   15 return (max-left, max-right; left-sum + right-sum)

   FIND-MAXIMUM-SUBARRAY.A; low; high/
   1 if high == low
   2 return (low, high, A[low]) // base case: only one element
   3 else mid = floor((low+high)/2)
   4  (left-low, left-high, left-sum) = FIND-MAXIMUM-SUBARRAY(A, low, mid)
   5  (right-low, right-high, right-sum) = FIND-MAXIMUM-SUBARRAY(A, mid, high)
   6  (cross-low; cross-high, cross-sum) = FIND-MAX-CROSSING-SUBARRAY(A, low, mid, high)
   7 if left-sum >= right-sum and left-sum >= cross-sum
   8 return (left-low, left-high, left-sum)
   9 elseif right-sum >= left-sum and right-sum >= cross-sum
   10 return (right-low, right-high, right-sum)
   11 else return (cross-low; cross-high, cross-sum)

   note: remember: mid = floor((low + high)/2)

   -  Analysis of the divide-and-conquer algorithm:

   For the above algorithm, we have the following analysis:

   line 1 - 3: O(1)
   line 4: T(n/2)
   line 5: T(n/2)
   line 6: T()

Chapter 6 Heap Sort

-  The (binary) head data structure is an array object that we can view as a nearly binary tree.

   Each node of the tree corresponds to an element of the array. The tree is completetly filled on all levels except possibly the lowest(??), which is filled from
   the left up to a point. An array A that represents a heap is an object with two attributes:

   1) A.length, which (as usual) gives the number of elements in the array, and 2) A.heap-size, which represents how many elements in the heap are stored within array A.

   Assuming the root of the tree is A[1], and given the index i of a node, we can easily compute the indices of its parents, left child and right child.

   PARENT(i)   -> return floor(i/2)
   LEFT(i)     -> return i*2
   RIGHT(2)    -> return i*2+1

   Note: if the root of the tree is A[0], we can easily compute the indices of its parents.

   PARENT(i)   -> return floor((i-1)/2)
   LEFT(i)     -> return i*2+1
   RIGHT(2)    -> return i*2+2

   There are two types of heaps:

   max-heap: A[PARENT(i)] >= A[i];  -> The largetst element of the heap is the root.
   min-heap: A[PARENT(i)] <= A[i];  -> The smallest element of the heap is the root.

   note: min-heaps are commonly implement priority queues.

   -  The MAX-HEAPIFY procedure, which runs in O(lg(n)) time, is the key to maintaining the max-heap property.
   -  The BUILD-MAX-HEAP procedure, which runs in linear time, produces a max-heap from an unordered input array.
   -  The HEAPSORT procedure, which runs in O(n*(lg(n))) time, sorts an array in place.
   -  The MAX-HEAP-INSERT , HEAP-EXTRACT-MAX, HEAP-INCREASE-KEY, and HEAP-MAXIMUM procedures, which run in O(lg(n))time, allow the heap
      data structure to implement a priority queue. 

-   Leaf nodes for a given binary tree with n nodes

    all the leaf nodes: floor(n/2)+1, floor(n/2)+2, ..., n.

    Note: if the root of the tree is A[0], we can easily compute the indices of all the leaf nodes

    all the leaf nodes: floor((n-1)/2)+1, floor((n-1)/2)+2, ..., n.

-   BUILD-MAX-HEAP(A):

    1. heap-size = A.length;
    2. for i = 1 to floor(n/2) downto 1
    3.    HEAPIFY(A, i)

    Note: This takes O(n).

-   HEAPSORT(A)

    1   BUILD-MAX-HEAP(A)
    2   for i = A.length downto 2
    3       exchange A[1] with A[i]
    4       A.heapsize = A.heapsize - 1
    5       MAX-HEAPIFY(A,1)

    Note: 

    line 5: MAX-HEAPIFY(A,1) takes O(logn)
    line 1: takes O(n).
    line 2 to 4 takes O(n-1)

    So HEAPSORT takes O(n*logn) + O(n) = O(n*logn)

-  In order to maintain the max-heap property, we call the procedure MAX-HEAPIFY. 

   MAX-HEAPIFY(A,i)
   1  l = LEFT(i)
   2  r = RIGHT(i)
   3  if l <= A.heap-size and A[l] > A[i]
   4     largest = l
   5  else largest = i
   6  if r<= A.heap-size and A[r] > A[largest]
   7     largest = r
   8  if largest != i
   9     exchange A[i] with A[largest]
   10    MAX-HEAPIFY(A, largest)

   Note: MAX-HEAPIFY(A,0) takes O(logn).

   The running time of MAX-HEAPIFY on a subtree of size n rooted at a given node i is the O(1) time to fix up the relationships among the elements A[i],
   A[LEFT(i)], and A[RIGHT(i)], plus the time to run MAX-HEAPIFY on a subtree rooted at one of the children of node i (assuming that the recursive call
   occurs). The children's subtrees each have size at most 2n/3 - the worst cases occurs when the bottom level of the tree is exactly half full - and therefore
   we can describe the running time of MAX-HEAPIFY by the recurrence

   T(n) <= T(2n/3) + O(1) (??)

   The solution to this recurrence, by case 2 of the master theorem (Theorem 4.1), is T(n) = O(lgn). Alternatively, we can characterize the running time of
   MAX-HEAPIFY on a node of height h as O(h).

-  Heapsort is an excellent algorithm, but a good implementation of quicksort, presented in Chapter 7, usually beats it in practice. Nevertheless, the heap
   data strucutre itself has many uses: priority queue

   -  A priority queue is a data structure for maintaining a set S of elements, each with an associated value called a key, A max-priority queue supports the
      following operations:

      INSERT(S, x): inserts the element x into the set S, which is equivalent to the operation S = S U {x}.
      MAXIMUM(s): returns the element of S with the largest key.
      EXTRACT-MAX(S): removes and returns the element of S with the largest key.
      INCREASE-KEY(S, x, k): increases the value of element x' key to the new value k, which is assumed to be at least as x's current key value.

    -   
      
      



Chapter 7 QuickSort

-  Like MergeSort, applies the divide-and-conquer paradigm. Here is the three-step divide-and-conquer process for sorting a typical subarray A[p..r]:

   Divide: Partition (rearrange the array) A[p..r] into two (possibly empty) subarrays A[p..r-1] and A[q+1..r]. 

-   The running time of quicksort depends on whether the partitioning is balanced or unbalanced, which in turn depends on which elements are used for partitioning.
    If the partitioning is balanced, the algorithm runs sysmptotically as fast as merge sort. If the partitioning is unbalanced, however, it can run asymptotically
    as slowly as insertion sort.

-  Maintaining the heap property

   In order to maintain the max-heap property, we call the procedure MAX-HEAPIFY:

   input: an array A and an index i into the array.   

   When it is called, MAX-HEAPIFY assumes that the binary tree rooted at LEFT(i)

Chapter 11 Hash Tables

-  The process of finding a record using some computation to map its key value to a position in the table is called Hashing.

-  The function that maps key values to positions is called hash function.

-  Hashing only works to store sets. That is, hashing cannot be used for applications where multiple records with the same key values are permitted. Hashing is not a good method
   for answering range searches. In other words, we cannot easily find all records (if any) whose key values fall within a certain range. Hashing is most appripriate for answering
   the question "What record, if any, has key value K?"

-  Many applications require a dynamic set that supports only the dictionary operation INSERT, SEARCH and DELETE. Although searching for an element data structure in a hash table
   can take as long as searching for an element in a link-list:O(n). In practice, hashing performs extremely well. Under resonable assumptions, the average time to search for an
   element in a hash table is O(1).

-  Finding a record with key value K in a database organized by hashing follows a two-step procedure:

   1) Compute the table location h(k);

   2) Starting with slot h(K), locate the record containing key K using (if necessary) a collision resolution policy.

-  Direct-address tables

   The Direct-address tables is used for a dynamic set in which each element has a key drawn from the universe U = {0, 1, ..., m-1} where m is not too large. We shall assume that
   no two elements have the same key(x1.key == x2.key, where x1 and x2 belong to U).

   The direct address table: T[0..m-1]
   slot k corresponds to a key in the set with key k. If the set contains no element with key k, then T[k] = NIL;
   
   The operations are trival to implement:

   1) DIRECT-ADDRESS-SEARCH(T,k)
   
   return T[k]

   2) DIRECT-ADDRESS-INSERT(T,x)

   T[x.key] = x;

   3) DIRECT-ADDRESS-DELETE(T,x)
   
   T[x.key] = NIL;

   Each operation takes O(1) time

-  Hash Tables

   Storing a table T of size |U| may be impractical, or even impossible due to memory limitation. With direct addressing, an element with key k is stored in slot k. With hashing,
   this element is stored in slot h(k); that is, we use a hash function h to compute the slot from the key k. Here, h maps the universe U of keys into the slots of a hash table
   T[0..m-1].

   h: U -> {0, 1, ..., m-1}.

   where the size of m of the hash table is typically much less than |U|. We also say that h(k) is the hash value of key k.

-  Two keys hash to the same slot. We call it collision. The dictionary operations on a hash table are easy to implement when collisions are resolved by chaining:

   1) CHAINED-HASH-INSERT(T,x) ---> Running time: O(1）, assuming the linked list is a double linked list.

   Insert x at the head of list T[h(x.key)].

   2) CHAINED-HASH-SEARCH(T,k) ---> The running time is O(1+alpha) where alpha = n/m.

   search for an element with key k in list T[h(k)].

   3) CHAINED-HASH-DELETE(T,x) ---> Running time: O(1), assuming the linked list is a double linked list.

   remove the element x from the list T[h(x.key)].

-  Hashing functions

   The goal of a hash function is to minimize collisions.

   A good hash function satisfies (approximately) the assumption of simple uniform hashing.

   If we know the distribution. For example, if we know that keys are random real numbers k independently and uniformly distributed in the range 0 <= k < 1, then the hash function.
   
   h(k) = floor(km);

-  Interpreting keys as natural numbers

   We can find a way to inteprete non-natural number keys as natural numbers. E.g.

   p = 112 and t = 116 in ASCII code. Thus

   ( 112 * 128 ) + 116 = 14452;

-  The division methods

   h(k) = k mod m;

   m should not be the power of 2 becasue the hash value becomes the p lowest-order bits of k. (m = 2.^p).

   A prime not too close to an exact power of 2 is often a good choice.

-  The multiplication methods

-  Universal hashing

-  Open addressing

   In open addressing, all elements occupy the hash table itself. That is, each table entry contains either an element of the dynamic set or NIL. No elements are stored outside
   the table, unlike in chaining. Thus in open addressing, the hash table can "fill up" so that no further insertions can be made; one consequence is that the load factor alpha
   can never exceed 1.

   To perform insertion using open addressing, we successively examine, or probe the hash table until we find an empty slot in which to put the key. Instead of being fixed in order
    0, 1,...,m-1 (which requires O(n) search time), the sequence of positions probed depends upon the key being inserted. 

   To determine which slots to probe, we extend the hash function to include the probe number (starting from 0) as a second input. Thus the hash function becomes:

   h: U x {0, 1,..., m-1} -> {0, 1, ..., m-1}
   
   With open addressing, we require that for every key k, the probe sequence

   <h(k,0), h(k,1), ...,h(k,m-1) be a permutation of <0,1,...,m-1>, so that every hash-table position is eventually considered as a slot for a new key as the table fills up.

   1) Insertion

   HASH-INSERT(T,k)
   1 i = 0
   2 repeat
   3  j = h(k,i)
   4  if T[j] == NIL
   5     T[j] = k
   6     return j
   7  else i = i + 1
   8  until i == m
   9  error "hash table overflow"

   2) Search
   
   The search can terminate (unsuccessfully) when it finds an empty slot, since k would have been inserted and not later in its probe sequence. The procedure HASH-SEARCH 
   takes as input a hash table T and a key k, returning j if it finds that slot j contains key k, or NIL if key k is not present in table T.

   HASH-SEARCH(T,k)
   1 i = 0 
   2 repeat 
   3  j = h(k,i)
   4  if T[j] == k
   5     return j
   6  i = i + 1
   7  until T[j] == NIL or i == m
   8  return NIL

   3) Deletion

   When we delete a key from slot i, we cannot simply mark that slot as empty by storing NIL in it. If we did, we might be unable to retrieve any key k during whose insertion 
   we had probed slot i and found it occupied. What we are going to do then?

   We can mark the removed slot as "DELETED" so for insertion operation, we need to modify it so that it can inserted to the slot marked as "DELETED". There is no need to change
   the search procedure as it will pass over DELETED values while searching.

   In our analysis, we assume uniform hashing: the probe sequence of each key is equally likely to be any of the m! permutations of <0, 1, ..., m - 1>.

-  Linear probing

   It is a closed hashing with no bucketing, and a collision resolution policy that can potentially use any slot in the hashtable. 

   INSERT:

   We can view any collision resolution method as generating a sequence of hash table slots that can potentially hold the record. The first slot in the sequence will be the home
   position. If the home is occupied, then go to the next slot in the sequence and so on.

   Given an ordinary funcion h': U -> {0,1,...,m-1}, which refers to as an auxiliary hash function, the method of linear probing uses the hash function:

   h(k) = ( h'(k) + i ) mod m

   Problem: primary clustering. Long runs of occupied slots build up, increasing the average search time. Clusters arise because an empty slot preceded by i full slots get filled
   next with probability (i+1)/m. Long runs of occupied slots tend to get longer, and the average search time increases.

   How can we avoid primary clustering?

-  One possible improvement might be to use linear probing, but to skip slots by a constant c other than 1. This would make the probe function:

   p(K, i) = ci. This is still not good. 

-  Quadratic probing

   h(k,i) = (h'(k) + c1*i + c2*i.^2 ) mod m

   where c1 and c2 are positive constants.

-  Double hashing

   h(k,i) = (h1(k) + ih2(k)) mod m , 
   

Chapter 15: Dynamic Programming

-  Unlike Divide-and-Conquer, Dynamic programming applies when subproblems overlap - that is, when subproblems share subsubproblems. A dynamic-programming algorithm
   solves each subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblem.

-  Developing a dynamic-programming algorithm follows a sequence of 4 steps:

   1. Characterize the structure of an optimal solution;
   2. Recursively define the value of an optimal solution;
   3. Compute the value of an optimal solution, typically in a bottom-up fashion;
   4. Construct an optimal solution from computed information.

   if we need only the value of an optimal solution, and not the solution itself, then we can omit step 4.

-  Rod-cutting problem:

   If an optimal solution cuts the rod into k pieces, for some 1<=k<=n, then a optimal decomposition 

   n = i1 + i2 + ... + ik of the rod into pieces length i1, i2, ..., ik provides maximum corresponding revenue

   rn = pi1 + pi2 + ... + pik.

   More generally, we can frame the values rn for n>= 1 in terms of optimal revenues from shorter rods:

   rn = max(pn, r1 + rn-1, r2 + rn-2..., rn-1 + r1).

   pn: corresponds to making no cuts at all and selling the rod of length as is.
   The other n-1 arguments to max correspond to the maximum revenue obtained    


   CUT-ROD(p,n)
   1  if n == 0
   2     return 0
   3  q = -inf
   4  for i = 1 to n
   5     q = max(q,p[i] + CUT-ROD(p,n-1))
   6  return q
  

Chapter 16: Greddy Algorithm

Chapter 36: Bloom Filter

-   What is it?

    Bloom filters are compact data structures for probabilistic representation of a set in order to support membership queries (i.e. queries that ask: 
    “Is element X in set Y?”).  This compact representation is the payoff for allowing a small rate of false positives in membership queries; that is, 
    queries might incorrectly recognize an element as member of the set.

    A Bloom Filter is an implementation of a set which allows a certain probability of false positive when determining if a given object is a member of the set
    , in return for a reduction of memory required for the set. It is essentially an extension of the idea of using hash codes as an index to a BitSet for
    the purpose of duplicate elimination.

    It effectively works as follows:

    -   We allocate m bits to represent the set data;
    -   We write a hash function that, instead of a single hash code, produces k hash codes for a given object;
    -   To add an object to the set, we derive bit indexes from all k hash codes and set those bits;
    -   To determine if an object is in the set, we again calculate the corresponding hash codes and bit indexes, and say that it is present if and only
        if all corresponding bits are set.

    The margin of error (or false positive rate) comes from the fact that we add more and more objects to the set, we increase the likelihood of "accidently" 
    setting a combination of bits that corresponds to an object that isn't actually in the set. In an example of using a bloom filter to represent a set of
    strings, we find that allocating 8 bits per item gives us a false positive rate of around 4.9% with 2 hash codes and 2.3% with 4 hash codes. Within certain
    parameters, we can achieve a certain false positive rate either by increasing the number of bits per item (hence overall memory used) or by increasing
    the number of hash codes hash codes (and hence CPU usage required per lookup).

    Bloom filters are useful for "weeding out" items. For example, when placed on top of a database query, they can be used to avoid database "hits" for items
    that we know in advance (because our Bloom filters reports that they are not present)  . 

    

-   Constructing Bloom Filters

    Consider a set of  n elements.  Bloom filters describe membership information of A using a bit vector V of length m. For this, k hash 
    functions, h1, h2,...,hk with hi = {X -> [1,m]}, are used as described below:

    The following procedure builds an m bits Bloom filter, corresponding to a set A and using  hash functions:

    Procedure BloomFilter(set A, hash_functions, integer m)
        filter = allocate m bits initialized to 0
        foreach ai in A:
            foreach hash function hj:
                filter[hj(ai)] = 1
            end foreach
        end foreach
        return filter

    Therefore, if ai is member of a set A, in the resulting Bloom filter V all bits obtained corresponding to the hashed values of ai are set to 1.  
    Testing for membership of an element elm is equivalent to testing that all corresponding bits of V are set:  

    Procedure MembershipTest (elm, filter, hash_functions) 
         foreach hash function hj:
            if filter[hj(elm)] != 1 return No
         end foreach
         return Yes

    Nice features:  filters can be built incrementally: as new elements are added to a set the corresponding positions are computed through the hash functions 
    and bits are set in the filter.  Moreover, the filter expressing the reunion of two sets is simply computed as the bit-wise OR applied over the two 
    corresponding Bloom filters.

-   Chapter 22: Consistent Hashing

    -    Web caching problem:

	    One classic problem: There are N machines that used as web content caching systems. Design an algorithm to balance the workload. That means distribute the
	    caching requests uniformally to each of N machines.

	    One simple way: hash(object)%N

	    There are two situations to think about the above solution:

	    1) if one cache system m is down, all the caching contents stored in m do not exist any more. Therefore, we need to remove machine m and redo all the mapping:

	    hash(object)%(N-1)

	    2) Because there are more demands for web caching, we need to add 1 more machine. So now the mapping becomes：

	    hash(object)%(N+1)

	    If 1) or 2) happens, suddenly all caching effects are gone! This is disastrous because all the requests are rushing to the backend server. 

    -    Monotonicity(??)

         Monotonicity is one of the benchmark to evaluate the good/bad of a hash function. What is that? If some contents have been hashed into their corresponding
         buckets, and some new buckets are being created, the hash function must safisfy: the original contents(not all of them) will be rehashed into those newly created buckets instead
	 of the other buckets in the old buckets poll.

    -    Balance

	 Balance is another benchmark to evaulate a hash function. It means to take the best effort to distribute the contents uniformally into the buckets.

    -    consistent hashing

	 1. round table architecture. (e.g. 2.^32 as the numerical bucket space)
         2. both contents and the cache share the same numerical bucket space. They are both dipicted as nodes at the rim of the round table.
         3. move the content clock-wisely until it hits a cache node.
	 4. if one cache node is B dead, all the content nodes which depend on B are moving clock-wisely to the next cache node C.
         5. if we need to add one cache node, we need to do remapping. All the nodes that are sitting between the new node and the new node's previous 
            cache node (counter-clockwisely)
	 6. To achieve high balance, we introduct intermediate nodes, so that one cache node can have multiple virtual nodes. And now the content nodes
	    map to virtual nodes instead of cache nodes. How can we compute virtual nodes's bucket locations? We can use the ip of the cache node plus
	    the suffix. E.g.

	    Assume cache A's ip address is: 202.168.14.241, the way we compute two virtual nodes for cache A is:

            Hash("202.168.14.241#1"); //cache A1
            Hash("202.168.14.241#2"); //cache A2

	Note: Please write a java program to demonstrate the consistent hash idea(!!)

-   Side notes

    -   Difference among HashMap, LinkedHashMap, and TreeMap

        -   HashMap

            It makes absolutely no guarantee about the iteration order. It can (and will) even change completely when new elements are added.

        -   TreeMap

            It will iterate according to the "natural ordering" of the keys according to their compareTo() method (or an externally supplied Comparator). 
            Additionally, it implements the SortedMap interface, which contains methods that depend on this sort order.

        -   LinkedHashMap

            It will iterate in the order in which the entries were put into the map. This implementation spares the clients from the unspecified, generally
            chaotic ordering provided by HashMap. Note that insertion order is not affected if a key is re-inserted into the map. (A key k is reinserted 
            into a map m if m.put(k, v) is invoked when m.contains(k) would return true immediately prior to the invocation).

            ╔══════════════╦═════════════════════╦═══════════════════╦══════════════════════╗
            ║   Property   ║       HashMap       ║      TreeMap      ║     LinkedHashMap    ║
            ╠══════════════╬═════════════════════╬═══════════════════╬══════════════════════╣
            ║              ║  no guarantee order ║ sorted according  ║                      ║
            ║   Order      ║ will remain constant║ to the natural    ║    insertion-order   ║
            ║              ║      over time      ║    ordering       ║                      ║
            ╠══════════════╬═════════════════════╬═══════════════════╬══════════════════════╣
            ║  Get/put     ║                     ║                   ║                      ║
            ║   remove     ║         O(1)        ║      O(log(n))    ║         O(1)         ║
            ║ containsKey  ║                     ║                   ║                      ║
            ╠══════════════╬═════════════════════╬═══════════════════╬══════════════════════╣
            ║              ║                     ║   NavigableMap    ║                      ║
            ║  Interfaces  ║         Map         ║       Map         ║         Map          ║
            ║              ║                     ║    SortedMap      ║                      ║
            ╠══════════════╬═════════════════════╬═══════════════════╬══════════════════════╣
            ║              ║                     ║                   ║                      ║
            ║     Null     ║       allowed       ║    only values    ║       allowed        ║
            ║ values/keys  ║                     ║                   ║                      ║
            ╠══════════════╬═════════════════════╩═══════════════════╩══════════════════════╣
            ║              ║   Fail-fast behavior of an iterator cannot be guaranteed       ║
            ║   Fail-fast  ║ impossible to make any hard guarantees in the presence of      ║
            ║   behavior   ║           unsynchronized concurrent modification               ║
            ╠══════════════╬═════════════════════╦═══════════════════╦══════════════════════╣
            ║              ║                     ║                   ║                      ║
            ║Implementation║      buckets        ║   Red-Black Tree  ║    double-linked     ║
            ║              ║                     ║                   ║       buckets        ║
            ╠══════════════╬═════════════════════╩═══════════════════╩══════════════════════╣
            ║      Is      ║                                                                ║
            ║ synchronized ║              implementation is not synchronized                ║
            ╚══════════════╩════════════════════════════════════════════════════════════════╝





-   Math

    One prominent feature of Bloom filters is that there is a clear tradeoff between the size of the filter and the rate of false positives.

-   Assignments

    2.2-2
    2.2-4
    2.3-5
    2-4

    3.1-2
    3.1-3
    3.1-4
    3.2-4

    4.2-4
    4.4-6
    4.4-9

    5.2-1
    5.2-4
    5.2-5
    5.3-2
    5.3-4

    6.1-1
    6.1-2
    6.2-6
    6.4-1
    6.5-2
    6-1

    7.2-3
    7.2-5

    8.1-3
    8.2-3
    8.3-3
    8.3-4
    8-1

    9.3-1
    9.3-3
    9,3-5
    9-1

    11.2-1
    11.2-4
    11-2

    12.1-2
    12.2-7
    12.3-3
    12-2

    13.1-4
    13.1-5
    13.3-3
    13-1

    14.1-7
    14.2-2
    14.3-7

    15.2-5
    15.3-1
    15.4-4
    15-4

    16.1-4
    16.2-2
    16.2-7

    17.1-3
    17.2-2
    17.2-3

    21.2-3
    21.2-6
    22.1-7
    22.2-5
    22.3-12
    22.4-3
    22-1

    23.1-1
    23.1-4
    23.1-6

    24.1-3
    24.3-3
    24.3-6
    24.4-7
    24.5-4
    24-3

    25.1-3
    25.1-5
    25.2-4
    25.3-4

    26.2-11
    26.3-3
    26-4
        
