Chapter 1: The Role of Algorithms in Computing

-  What is algorithm?

   Informally, an algorithm is any well-defined computational procedure that takes
   some value, or set of values, as input and produces some value, or set of values, as
   output. An algorithm is thus a sequence of computational steps that transform the
   input into the output.

-  What is data structure?

   A data structure is a way to store and organize data in order to facilitate access
   and modifications.
   
Chapter 2: Getting started

-  One example: Insert Sort ( it takes time roughly equal to c1*n.^2 )

   for j = 2 to A.length
      key = A[j]
      /* Insert A[j] into the sorted sequence A[1...j-1] */
      i = j - 1
      while i > 0 and A[i] > key
         A[i+1] = A[i]
         i = i - 1
      A[i+1] = key   <---This line doesn't belong to the above while loop (line 23 to line 25)

-  Loop variants

   Loop variants are to help us understand why an algorithm is correct:

   Initialization: It is true prior to the first iteration of the loop.

   Maintenance: If it is true before an iteration of the loop, it remains true before the next iteration.

   Termination: When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.

-  2.1-3

   Input:   A sequence of n numbers A = <a1, a2, ..., an> and a value v.
   Output:  An index i such that v = A[i] or the special value NIL if v does not appear in A

   Write pseudocode for linear search, which scans through the sequence, looking
   for . Using a loop invariant, prove that your algorithm is correct. Make sure that
   your loop invariant fulfills the three necessary properties.

   v = NIL
   for i = 1 to A.length
      if v = A[i]
      v = A[i]
      return
   
   Initialization: v=NIL, i = 1.
   Maintenance: i<=A.length, or, v != A[i] while i <= A.length.
   Termination: v=A[i] while i < A.length+1, or, i=A.length+1

-  Analyzing algorithms

   Running time: the running time of an algorithm on a particular input is the number of primitive operations or "steps" executed. 

   Loop running time: When a for or while loop exits in the usual way (i.e. due to the test in the loop header), the test is executed one time more than the loop body.

   Worst case running time: We shall usually concentrate on finding only the worst-case running time, that is, the longest running time for any input of size n.

   Order of growth: To make analizing algorithms more simplified, we use the rate of growth,or order of growth, of the running time that really interests us. E.g.

   Insert Sort's running time (of course in the worst case)is a*n.^2 + b*n + c. For a given large value of n, b*n and c are relatively insignificant so we consider only
   the leading term of a formula: a*n.^2. We also ignore the leading term's constant coefficient, since constant factors are less significant for large values of n. In summary,
   We use "theata of n-squared" to represent the running time of insert sort: O(n.^2). 

   We usually consider one algorithm to be more efficient than another if its worst-case running time has a lower order of growth. Due to constant factors and lower-order terms,
   an algorithm whose running time has a higher order of growth might take less time for small inputs than an apgorithm whose running time has a lower order of growth. But for large
   inputs, a O(n.^2) algorithm, for example, will run more quickly in the worst case than a O(n.^3) algorithm.

-  Designing algorithms

   The divide-and-conquer approach: Recursion functions are one of those functions that can call itself recursively one or more time to deal with closely related sub problems.

   Divide: divide the problem into a number of subproblems that are smaller instances of the same problem.
   
   Conquer: Conquer the subproblems by solving them recursively. If the subproblems are sizes are small enough, however, just solve the subproblems in a straightforward manner.

   Combine: Combine the solutions to the subproblems into the solution for the original problem.

   E.g. Merge sort:

   Divide: Divide the n-element sequence to be sorted into two subsequences of n/2 elements each.
   Conquer: Sort the two subsequences recursively using merge sort.
   Combine: Merge the two sorted subsequences to produce the sorted answer.

   *  Merge:

   1   MERGE(A, p, q, r)
   1   n1 = q-p+1
   2   n2 = r-q
   3   Let L[1, ...n1] and R[1, n2] be the new two arrays.
   4   for i = 1 to n1
   5      L[i] = A[p-1+i]
   6   for j = 1 to n2
   7      R[j] = A[q+j]
   8   L[n1+1]=inf
   9   R[n2+1]=inf
   10  i = 1
   11  j = 1
   12  for k = p to r
   13    if L[i]<=R[j]
   14       A[k] = L[i]
   15       i = i + 1
   16    else
   17       A[k] = R[j]
   18       j = j + 1

   *  Merge_Sort:

   MERGE-SORT(A, p, r)
 
   1  if (p < r)
   2  q = floor((p+r)/2)
   3  MERGE-SORT(A, p, q)
   4  MERGE-SORT(A, q+1, r)
   5  MERGE(A, p, q, r)

   Typically, we set p = 1 and r = n. Now we analyze divide-and-conquer algorithms.

   When an algorithm contains a recursive call to itself, we can often describe its running time by a recurrence equation or recurrence, which describes the overall running time
   ona a problem of size n in terms of the running time on smaller inputs. 

   Tn = o(1)                           if n <= c
      = a*T(n/b) + D(n) + C(n)         otherwise
   
   1) If the problem is smaller enough (e.g. sorting two numbers), say n<=c, for some constant c, the straightforward solution takes constant time, which we write o(1).
   2) Suppose that our division of the problem yields a subproblems, each of which is 1=b the size of the original. (For merge sort, both a and b are 2, each of which is 
   the size of the original. (For merge sort, both  andbut we shall see many divide-and-conquer algorithms in which a Â¤ b.) time T .n=b/ to solve one subproblem of size 
   n=b, and so it takes time a*T(n/b).
   3) If we take D(n) time to divide the problem into subproblems and C(n) time to combine the solutions to the subproblems into the original problem. 

   * Analysis of Merge Sort

   
